{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rental_Predictions_Zillow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python39164bitcityspireadspipenvfc59c3d301b9445a835af572902f875d",
      "display_name": "Python 3.9.1 64-bit ('cityspire-a-ds': pipenv)",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBlKy9mmHOIu"
      },
      "source": [
        "# Zillow Dataset Cleaning - First Step\n",
        " Use this as a reference, you **do not** need to run these cells\n",
        " - [Link to the orginal dataset](https://www.zillow.com/research/data/) from Zillow's website\n",
        " - Select: `ZORI All Homes Plus Multifamily Smoothed` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do5epFXcHGDs"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m__TbrwoR5DI"
      },
      "source": [
        "# Connect to data\n",
        "FILEPATH = 'https://raw.githubusercontent.com/Lambda-School-Labs/cityspire-a-ds/main/notebooks/Rental_Data/Original_Zillow.csv'\n",
        "zillow_original = pd.read_csv(FILEPATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F09oXa5kXDaN"
      },
      "source": [
        "# Function to clean up the dataset + yearly average columns\n",
        "\n",
        "def zillow_cleaner(df):\n",
        "  ''' A function to convert a dataset from Zillow into something more useable \n",
        "  for a predictive model.\n",
        "  '''\n",
        "\n",
        "  # Drop columns\n",
        "  df = df.drop(['RegionID', 'RegionName', 'SizeRank'], axis=1)\n",
        "\n",
        "  # Change column name\n",
        "  df = df.rename(columns={\"MsaName\": \"City/State\"})\n",
        "\n",
        "  # Creating yearly average columns\n",
        "  # 2014 averages\n",
        "  fourteen_columns = list(df)[1:13]\n",
        "  df.insert(13, '2014_Average',\n",
        "                    df[fourteen_columns].mean(axis=1), True)\n",
        "  # 2015 averages\n",
        "  fifteen_columns = list(df)[14:26]\n",
        "  df.insert(26, '2015_Average',\n",
        "                    df[fifteen_columns].mean(axis=1), True)\n",
        "  # 2016 averages\n",
        "  sixteen_columns = list(df)[27:39]\n",
        "  df.insert(39, '2016_Average',\n",
        "                    df[sixteen_columns].mean(axis=1), True)\n",
        "  # 2017 averages\n",
        "  seventeen_columns = list(df)[40:52]\n",
        "  df.insert(52, '2017_Average',\n",
        "                    df[seventeen_columns].mean(axis=1), True)\n",
        "  # 2018 averages\n",
        "  eightteen_columns = list(df)[53:65]\n",
        "  df.insert(65, '2018_Average',\n",
        "                    df[eightteen_columns].mean(axis=1), True)\n",
        "  # 2019 averages\n",
        "  nineteen_columns = list(df)[66:78]\n",
        "  df.insert(78, '2019_Average',\n",
        "                    df[nineteen_columns].mean(axis=1), True)\n",
        "  # 2020 averaes\n",
        "  twentytwenty_columns = list(df)[79:91]\n",
        "  df.insert(91, '2020_Average',\n",
        "                    df[twentytwenty_columns].mean(axis=1), True)\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bsQO99SXTLy"
      },
      "source": [
        "# Run cleaning function\n",
        "zillow_original = zillow_cleaner(zillow_original)\n",
        "\n",
        "# Check it out\n",
        "print(zillow_original.shape)\n",
        "zillow_original.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1TsKZOxYIN0"
      },
      "source": [
        "[GeekforGeeks](https://www.geeksforgeeks.org/python-pandas-dataframe-interpolate) - Interpolate Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRKIY-7vXb8N"
      },
      "source": [
        "# Deal with NaN with interpolate, a way to estimate instead of dropping\n",
        "zillow_original.interpolate(method='linear', axis=0, inplace=True, limit_direction='both', limit_area='inside', downcast=None)\n",
        "\n",
        "# Inspect\n",
        "zillow_original.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS2HrRL8Y7YK"
      },
      "source": [
        "Oddly not all of the NaNs were converted to a numerical value based on neighbors. Assumption is that if there are multiple NaNs in sequence the `interpolate()` function has a hard time defining a value.\n",
        "\n",
        "There were only a hand full of these, so they were corrected manually. Taking the average of the nearest four neighbors, a numerical value was given for the NaN."
      ]
    },
    {
      "source": [
        "# Zillow Dataset Cleaning - Phase 2\n",
        "- The dataset below has the NaNs correctly manually"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to data\n",
        "FILEPATH = 'https://raw.githubusercontent.com/Lambda-School-Labs/cityspire-a-ds/main/notebooks/Rental_Data/Corrected_Zillow.csv'\n",
        "zillow_corrected = pd.read_csv(FILEPATH)"
      ]
    },
    {
      "source": [
        "## Step 1: break apart the state from the city list\n",
        "\n",
        "This dataset contains multiple cities that are in the same observations. For example *Dallas-Fort Worth*\n",
        "\n",
        "Thus `explode` is used to seperate the observation into two rows. Giving the model more data to work with.\n",
        "\n",
        "In order to use this function, the `City/State` columns has to be two seperate columns\n",
        "\n",
        "[Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html) - *Explode Function*"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split 'city/state' so the cities can be exploded\n",
        "zillow_corrected[['City', 'State']] = zillow_corrected['City/State'].str.split(', ', expand=True)\n",
        " \n",
        "# This is being dropped - changing the name later\n",
        "zillow_corrected = zillow_corrected.drop('City/State', axis=1)\n",
        " \n",
        "# Explode the rows with multiple cities in an observation\n",
        "zillow_corrected['City'] = zillow_corrected['City'].str.split('-')\n",
        "zillow_corrected = zillow_corrected.explode('City')"
      ]
    },
    {
      "source": [
        "## Step 2: Convert abbreviated state names into full names\n",
        "- [GitHub](https://gist.githubusercontent.com/rogerallen/1583593/raw/0fffdee6149ab1d993dffa51b1fa9aa466704e18/us_state_abbrev.py) - US State dictionary used below"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "us_state_abbrev = {\n",
        "    'Alabama': 'AL', 'Alaska': 'AK', 'American Samoa': 'AS', 'Arizona': 'AZ',\n",
        "    'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT',\n",
        "    'Delaware': 'DE', 'District of Columbia': 'DC', 'Florida': 'FL',\n",
        "    'Georgia': 'GA', 'Guam': 'GU', 'Hawaii': 'HI', 'Idaho': 'ID',\n",
        "    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
        "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
        "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN',\n",
        "    'Mississippi': 'MS', 'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE',\n",
        "    'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n",
        "    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC',\n",
        "    'North Dakota': 'ND', 'Northern Mariana Islands':'MP', 'Ohio': 'OH',\n",
        "    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Puerto Rico': 'PR',\n",
        "    'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD',\n",
        "    'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n",
        "    'Virgin Islands': 'VI', 'Virginia': 'VA', 'Washington': 'WA',\n",
        "    'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
        "}\n",
        " \n",
        "# Flip the dictonary, need the state codes as keys\n",
        "us_state_abbrev = {value: key for key, value in us_state_abbrev.items()}\n",
        " \n",
        "# Convert the state codes to state names\n",
        "zillow_corrected['State'] = zillow_corrected['State'].map(us_state_abbrev)\n"
      ]
    },
    {
      "source": [
        "## Step 3: Re-combine the city and state information\n",
        "\n",
        "- New column will be `City_State`"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine and insert at the front of the dataframe\n",
        "zillow_corrected.insert(loc=0, column='City_State', \n",
        "                        value=(zillow_corrected['City'] + ', ' + \n",
        "                               zillow_corrected['State']))\n",
        " \n",
        "# Delete our temp columns\n",
        "zillow_corrected = zillow_corrected.drop(['City', 'State'], axis=1)"
      ]
    },
    {
      "source": [
        "## Step 4: Handle duplicate city-state entries\n",
        "\n",
        "The original dataset has multiple observations for the same city. The reason behind this is due to a city having multiple zipcodes, thus data was collected by zipcode.\n",
        "\n",
        "Grouping these observations together into one row is ideal. Taking the average of the combined cities zipcodes helps keep the most data.\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average accross the city \n",
        "zillow_corrected = zillow_corrected.groupby('City_State').mean()\n",
        "\n",
        "# Round to 2 decimal places\n",
        "zillow_corrected = zillow_corrected.round(decimals=2)"
      ]
    }
  ]
}