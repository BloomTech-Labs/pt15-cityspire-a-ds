{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rental_Predictions_Zillow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python39164bitcityspireadspipenvfc59c3d301b9445a835af572902f875d",
      "display_name": "Python 3.9.1 64-bit ('cityspire-a-ds': pipenv)",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBlKy9mmHOIu"
      },
      "source": [
        "# Zillow Dataset Cleaning - Phase 1\n",
        " Use this as a reference, you **do not** need to run these cells in Phase 1\n",
        " - [Link to the orginal dataset](https://www.zillow.com/research/data/) from Zillow's website\n",
        " - Select: `ZORI All Homes Plus Multifamily Smoothed` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do5epFXcHGDs"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m__TbrwoR5DI"
      },
      "source": [
        "# Connect to data\n",
        "FILEPATH = 'https://raw.githubusercontent.com/Lambda-School-Labs/cityspire-a-ds/main/notebooks/Rental_Data/Zillow_Datasets/Zillow_Original.csv'\n",
        "zillow_original = pd.read_csv(FILEPATH)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F09oXa5kXDaN"
      },
      "source": [
        "# Function to clean up the dataset + yearly average columns\n",
        "\n",
        "def zillow_cleaner(df):\n",
        "  ''' A function to convert a dataset from Zillow into something more useable \n",
        "  for a predictive model.\n",
        "  '''\n",
        "\n",
        "  # Drop columns\n",
        "  df = df.drop(['RegionID', 'RegionName', 'SizeRank'], axis=1)\n",
        "\n",
        "  # Change column name\n",
        "  df = df.rename(columns={\"MsaName\": \"City_State\"})\n",
        "\n",
        "  # Creating yearly average columns\n",
        "  # 2014 averages\n",
        "  fourteen_columns = list(df)[1:13]\n",
        "  df.insert(13, '2014_Average',\n",
        "                    df[fourteen_columns].mean(axis=1), True)\n",
        "  # 2015 averages\n",
        "  fifteen_columns = list(df)[14:26]\n",
        "  df.insert(26, '2015_Average',\n",
        "                    df[fifteen_columns].mean(axis=1), True)\n",
        "  # 2016 averages\n",
        "  sixteen_columns = list(df)[27:39]\n",
        "  df.insert(39, '2016_Average',\n",
        "                    df[sixteen_columns].mean(axis=1), True)\n",
        "  # 2017 averages\n",
        "  seventeen_columns = list(df)[40:52]\n",
        "  df.insert(52, '2017_Average',\n",
        "                    df[seventeen_columns].mean(axis=1), True)\n",
        "  # 2018 averages\n",
        "  eightteen_columns = list(df)[53:65]\n",
        "  df.insert(65, '2018_Average',\n",
        "                    df[eightteen_columns].mean(axis=1), True)\n",
        "  # 2019 averages\n",
        "  nineteen_columns = list(df)[66:78]\n",
        "  df.insert(78, '2019_Average',\n",
        "                    df[nineteen_columns].mean(axis=1), True)\n",
        "  # 2020 averaes\n",
        "  twentytwenty_columns = list(df)[79:91]\n",
        "  df.insert(91, '2020_Average',\n",
        "                    df[twentytwenty_columns].mean(axis=1), True)\n",
        "\n",
        "  return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bsQO99SXTLy"
      },
      "source": [
        "# Run cleaning function\n",
        "zillow_original = zillow_cleaner(zillow_original)\n",
        "\n",
        "# Check it out\n",
        "print(zillow_original.shape)\n",
        "zillow_original.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(106, 92)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   2014-01  2014-02  2014-03  2014-04  2014-05  2014-06  2014-07  2014-08  \\\n",
              "0   1341.0   1346.0   1353.0   1366.0   1386.0     1401     1412     1418   \n",
              "1   2280.0   2299.0   2303.0   2334.0   2369.0     2391     2405     2414   \n",
              "2   1789.0   1811.0   1820.0   1848.0   1884.0     1918     1943     1959   \n",
              "3   1403.0   1414.0   1416.0   1432.0   1453.0     1469     1478     1481   \n",
              "4   1167.0   1157.0   1168.0   1186.0   1207.0     1224     1232     1239   \n",
              "\n",
              "   2014-09  2014-10  ...  2020-05  2020-06  2020-07  2020-08  2020-09  \\\n",
              "0     1419     1418  ...     1723     1725     1725     1716     1713   \n",
              "1     2415     2414  ...     2715     2698     2666     2602     2551   \n",
              "2     1966     1975  ...     2527     2533     2532     2527     2524   \n",
              "3     1478     1470  ...     1676     1678     1672     1659     1640   \n",
              "4     1240     1237  ...     1524     1529     1534     1533     1534   \n",
              "\n",
              "   2020-10  2020-11  2020-12  2021-01  2020_Average  \n",
              "0     1707     1715   1715.0   1721.0   1719.500000  \n",
              "1     2496     2471   2450.0   2465.0   2608.916667  \n",
              "2     2525     2539   2533.0   2542.0   2538.250000  \n",
              "3     1623     1615   1611.0   1614.0   1651.250000  \n",
              "4     1537     1550   1555.0   1555.0   1537.333333  \n",
              "\n",
              "[5 rows x 92 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>2014-01</th>\n      <th>2014-02</th>\n      <th>2014-03</th>\n      <th>2014-04</th>\n      <th>2014-05</th>\n      <th>2014-06</th>\n      <th>2014-07</th>\n      <th>2014-08</th>\n      <th>2014-09</th>\n      <th>2014-10</th>\n      <th>...</th>\n      <th>2020-05</th>\n      <th>2020-06</th>\n      <th>2020-07</th>\n      <th>2020-08</th>\n      <th>2020-09</th>\n      <th>2020-10</th>\n      <th>2020-11</th>\n      <th>2020-12</th>\n      <th>2021-01</th>\n      <th>2020_Average</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1341.0</td>\n      <td>1346.0</td>\n      <td>1353.0</td>\n      <td>1366.0</td>\n      <td>1386.0</td>\n      <td>1401</td>\n      <td>1412</td>\n      <td>1418</td>\n      <td>1419</td>\n      <td>1418</td>\n      <td>...</td>\n      <td>1723</td>\n      <td>1725</td>\n      <td>1725</td>\n      <td>1716</td>\n      <td>1713</td>\n      <td>1707</td>\n      <td>1715</td>\n      <td>1715.0</td>\n      <td>1721.0</td>\n      <td>1719.500000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2280.0</td>\n      <td>2299.0</td>\n      <td>2303.0</td>\n      <td>2334.0</td>\n      <td>2369.0</td>\n      <td>2391</td>\n      <td>2405</td>\n      <td>2414</td>\n      <td>2415</td>\n      <td>2414</td>\n      <td>...</td>\n      <td>2715</td>\n      <td>2698</td>\n      <td>2666</td>\n      <td>2602</td>\n      <td>2551</td>\n      <td>2496</td>\n      <td>2471</td>\n      <td>2450.0</td>\n      <td>2465.0</td>\n      <td>2608.916667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1789.0</td>\n      <td>1811.0</td>\n      <td>1820.0</td>\n      <td>1848.0</td>\n      <td>1884.0</td>\n      <td>1918</td>\n      <td>1943</td>\n      <td>1959</td>\n      <td>1966</td>\n      <td>1975</td>\n      <td>...</td>\n      <td>2527</td>\n      <td>2533</td>\n      <td>2532</td>\n      <td>2527</td>\n      <td>2524</td>\n      <td>2525</td>\n      <td>2539</td>\n      <td>2533.0</td>\n      <td>2542.0</td>\n      <td>2538.250000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1403.0</td>\n      <td>1414.0</td>\n      <td>1416.0</td>\n      <td>1432.0</td>\n      <td>1453.0</td>\n      <td>1469</td>\n      <td>1478</td>\n      <td>1481</td>\n      <td>1478</td>\n      <td>1470</td>\n      <td>...</td>\n      <td>1676</td>\n      <td>1678</td>\n      <td>1672</td>\n      <td>1659</td>\n      <td>1640</td>\n      <td>1623</td>\n      <td>1615</td>\n      <td>1611.0</td>\n      <td>1614.0</td>\n      <td>1651.250000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1167.0</td>\n      <td>1157.0</td>\n      <td>1168.0</td>\n      <td>1186.0</td>\n      <td>1207.0</td>\n      <td>1224</td>\n      <td>1232</td>\n      <td>1239</td>\n      <td>1240</td>\n      <td>1237</td>\n      <td>...</td>\n      <td>1524</td>\n      <td>1529</td>\n      <td>1534</td>\n      <td>1533</td>\n      <td>1534</td>\n      <td>1537</td>\n      <td>1550</td>\n      <td>1555.0</td>\n      <td>1555.0</td>\n      <td>1537.333333</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 92 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1TsKZOxYIN0"
      },
      "source": [
        "[GeekforGeeks](https://www.geeksforgeeks.org/python-pandas-dataframe-interpolate) - Interpolate Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRKIY-7vXb8N"
      },
      "source": [
        "# Deal with NaN with interpolate, a way to estimate instead of dropping\n",
        "zillow_original.interpolate(method='linear', axis=0, inplace=True, limit_direction='both', limit_area='inside', downcast=None)\n",
        "\n",
        "# Inspect\n",
        "zillow_original.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 106 entries, 0 to 105\nData columns (total 92 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   2014-01       106 non-null    float64\n 1   2014-02       106 non-null    float64\n 2   2014-03       106 non-null    float64\n 3   2014-04       106 non-null    float64\n 4   2014-05       106 non-null    float64\n 5   2014-06       106 non-null    int64  \n 6   2014-07       106 non-null    int64  \n 7   2014-08       106 non-null    int64  \n 8   2014-09       106 non-null    int64  \n 9   2014-10       106 non-null    int64  \n 10  2014-11       106 non-null    int64  \n 11  2014-12       106 non-null    int64  \n 12  2015-01       106 non-null    int64  \n 13  2014_Average  106 non-null    float64\n 14  2015-02       106 non-null    int64  \n 15  2015-03       106 non-null    int64  \n 16  2015-04       106 non-null    int64  \n 17  2015-05       106 non-null    int64  \n 18  2015-06       106 non-null    int64  \n 19  2015-07       106 non-null    int64  \n 20  2015-08       106 non-null    int64  \n 21  2015-09       106 non-null    int64  \n 22  2015-10       106 non-null    int64  \n 23  2015-11       106 non-null    int64  \n 24  2015-12       106 non-null    int64  \n 25  2016-01       106 non-null    int64  \n 26  2015_Average  106 non-null    float64\n 27  2016-02       106 non-null    int64  \n 28  2016-03       106 non-null    int64  \n 29  2016-04       106 non-null    int64  \n 30  2016-05       106 non-null    int64  \n 31  2016-06       106 non-null    int64  \n 32  2016-07       106 non-null    int64  \n 33  2016-08       106 non-null    int64  \n 34  2016-09       106 non-null    int64  \n 35  2016-10       106 non-null    int64  \n 36  2016-11       106 non-null    int64  \n 37  2016-12       106 non-null    int64  \n 38  2017-01       106 non-null    int64  \n 39  2016_Average  106 non-null    float64\n 40  2017-02       106 non-null    int64  \n 41  2017-03       106 non-null    int64  \n 42  2017-04       106 non-null    int64  \n 43  2017-05       106 non-null    int64  \n 44  2017-06       106 non-null    int64  \n 45  2017-07       106 non-null    int64  \n 46  2017-08       106 non-null    int64  \n 47  2017-09       106 non-null    int64  \n 48  2017-10       106 non-null    int64  \n 49  2017-11       106 non-null    int64  \n 50  2017-12       106 non-null    int64  \n 51  2018-01       106 non-null    int64  \n 52  2017_Average  106 non-null    float64\n 53  2018-02       106 non-null    int64  \n 54  2018-03       106 non-null    int64  \n 55  2018-04       106 non-null    int64  \n 56  2018-05       106 non-null    int64  \n 57  2018-06       106 non-null    int64  \n 58  2018-07       106 non-null    int64  \n 59  2018-08       106 non-null    int64  \n 60  2018-09       106 non-null    int64  \n 61  2018-10       106 non-null    int64  \n 62  2018-11       106 non-null    int64  \n 63  2018-12       106 non-null    int64  \n 64  2019-01       106 non-null    float64\n 65  2018_Average  106 non-null    float64\n 66  2019-02       106 non-null    float64\n 67  2019-03       106 non-null    int64  \n 68  2019-04       106 non-null    int64  \n 69  2019-05       106 non-null    int64  \n 70  2019-06       106 non-null    int64  \n 71  2019-07       106 non-null    int64  \n 72  2019-08       106 non-null    int64  \n 73  2019-09       106 non-null    int64  \n 74  2019-10       106 non-null    int64  \n 75  2019-11       106 non-null    int64  \n 76  2019-12       106 non-null    int64  \n 77  2020-01       106 non-null    int64  \n 78  2019_Average  106 non-null    float64\n 79  2020-02       106 non-null    int64  \n 80  2020-03       106 non-null    int64  \n 81  2020-04       106 non-null    int64  \n 82  2020-05       106 non-null    int64  \n 83  2020-06       106 non-null    int64  \n 84  2020-07       106 non-null    int64  \n 85  2020-08       106 non-null    int64  \n 86  2020-09       106 non-null    int64  \n 87  2020-10       106 non-null    int64  \n 88  2020-11       106 non-null    int64  \n 89  2020-12       106 non-null    float64\n 90  2021-01       106 non-null    float64\n 91  2020_Average  106 non-null    float64\ndtypes: float64(16), int64(76)\nmemory usage: 76.3 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS2HrRL8Y7YK"
      },
      "source": [
        "Oddly not all of the NaNs were converted to a numerical value based on neighbors. Assumption is that if there are multiple NaNs in sequence the `interpolate()` function has a hard time defining a value.\n",
        "\n",
        "There were only a hand full of these, so they were corrected manually. Taking the average of the nearest four neighbors, a numerical value was given for the NaN."
      ]
    },
    {
      "source": [
        "# Zillow Dataset Cleaning - Phase 2\n",
        "- The dataset below has the NaNs correctly manually"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'category_encoders'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b61497da564d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from category_encoders import OrdinalEncoder\n",
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to data\n",
        "FILEPATH = 'https://raw.githubusercontent.com/Lambda-School-Labs/cityspire-a-ds/main/notebooks/Rental_Data/Zillow_Datasets/Corrected_Zillow.csv'\n",
        "zillow_corrected = pd.read_csv(FILEPATH)"
      ]
    },
    {
      "source": [
        "## Step 1: break apart the state from the city list\n",
        "\n",
        "This dataset contains multiple cities that are in the same observations. For example *Dallas-Fort Worth*\n",
        "\n",
        "Thus `explode` is used to seperate the observation into two rows. Giving the model more data to work with.\n",
        "\n",
        "In order to use this function, the `City/State` columns has to be two seperate columns\n",
        "\n",
        "[Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html) - *Explode Function*"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split 'city/state' so the cities can be exploded\n",
        "zillow_corrected[['City', 'State']] = zillow_corrected['City_State'].str.split(', ', expand=True)\n",
        " \n",
        "# This is being dropped - changing the name later\n",
        "zillow_corrected = zillow_corrected.drop('City_State', axis=1)\n",
        " \n",
        "# Explode the rows with multiple cities in an observation\n",
        "zillow_corrected['City'] = zillow_corrected['City'].str.split('-')\n",
        "zillow_corrected = zillow_corrected.explode('City')"
      ]
    },
    {
      "source": [
        "## Step 2: Convert abbreviated state names into full names\n",
        "- [GitHub](https://gist.githubusercontent.com/rogerallen/1583593/raw/0fffdee6149ab1d993dffa51b1fa9aa466704e18/us_state_abbrev.py) - US State dictionary used below"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "us_state_abbrev = {\n",
        "    'Alabama': 'AL', 'Alaska': 'AK', 'American Samoa': 'AS', 'Arizona': 'AZ',\n",
        "    'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT',\n",
        "    'Delaware': 'DE', 'District of Columbia': 'DC', 'Florida': 'FL',\n",
        "    'Georgia': 'GA', 'Guam': 'GU', 'Hawaii': 'HI', 'Idaho': 'ID',\n",
        "    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
        "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
        "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN',\n",
        "    'Mississippi': 'MS', 'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE',\n",
        "    'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n",
        "    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC',\n",
        "    'North Dakota': 'ND', 'Northern Mariana Islands':'MP', 'Ohio': 'OH',\n",
        "    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Puerto Rico': 'PR',\n",
        "    'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD',\n",
        "    'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n",
        "    'Virgin Islands': 'VI', 'Virginia': 'VA', 'Washington': 'WA',\n",
        "    'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
        "}\n",
        " \n",
        "# Flip the dictonary, need the state codes as keys\n",
        "us_state_abbrev = {value: key for key, value in us_state_abbrev.items()}\n",
        " \n",
        "# Convert the state codes to state names\n",
        "zillow_corrected['State'] = zillow_corrected['State'].map(us_state_abbrev)\n"
      ]
    },
    {
      "source": [
        "## Step 3: Re-combine the city and state information\n",
        "\n",
        "- New column will be `City_State`"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine and insert at the front of the dataframe\n",
        "zillow_corrected.insert(loc=0, column='City_State', \n",
        "                        value=(zillow_corrected['City'] + ', ' + \n",
        "                               zillow_corrected['State']))\n",
        " \n",
        "# Delete our temp columns\n",
        "zillow_corrected = zillow_corrected.drop(['City', 'State'], axis=1)"
      ]
    },
    {
      "source": [
        "## Step 4: Handle duplicate city-state entries\n",
        "\n",
        "The original dataset has multiple observations for the same city. The reason behind this is due to a city having multiple zipcodes, thus data was collected by zipcode.\n",
        "\n",
        "Grouping these observations together into one row is ideal. Taking the average of the combined cities zipcodes helps keep the most data.\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average accross the city \n",
        "zillow_corrected = zillow_corrected.groupby('City_State').mean()\n",
        "\n",
        "# Round to 2 decimal places\n",
        "zillow_corrected = zillow_corrected.round(decimals=2)"
      ]
    },
    {
      "source": [
        "# Modeling\n",
        "\n",
        "## Step 1: Separating Columns\n",
        "\n",
        "The dataset contains columns of months and columns for yearly averages. For the predictive model, the monthly columns will be used. The yearly averages will be used later on.\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the averages columns for later\n",
        "zillow_year_average = zillow_corrected[[col for col in zillow_corrected if 'Average' in col]]\n",
        "\n",
        "# These non-average columns are for predicting\n",
        "zillow_corrected = zillow_corrected[[col for col in zillow_corrected if 'Average' not in col]]"
      ]
    },
    {
      "source": [
        "## Step 2: Create the Model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'make_pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-307734882c3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define algorithms for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = make_pipeline(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'make_pipeline' is not defined"
          ]
        }
      ],
      "source": [
        "X_train = zillow_corrected.iloc[:, :-1]   # Every month but the last month\n",
        "y_test = zillow_corrected.iloc[:, -1:]    # Only the last month\n",
        "\n",
        "# Define algorithms for the model\n",
        "model = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    StandardScaler(),\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "model.fit(X_train, y_test)"
      ]
    },
    {
      "source": [
        "## Step 3: Check Metrics"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# New dataframe to display metrics\n",
        "test_results = pd.DataFrame(index=zillow_corrected.index)\n",
        "\n",
        "# Original results\n",
        "test_results['original'] = y_test\n",
        "\n",
        "# Predicted results from model - rounded\n",
        "test_results['predicted'] = np.round(model.predict(X_train), 2)\n",
        "\n",
        "# Take the difference from predicted vs original\n",
        "test_results['differnce'] = test_results['predicted'] - test_results['original']\n",
        "\n",
        "# Calculate the percent of error\n",
        "test_results['percent_error'] = test_results['differnce'].abs() / test_results['original'] * 100\n",
        "\n",
        "test_results"
      ]
    },
    {
      "source": [
        "# Phase 3 - Get Future Predictions\n",
        "\n",
        "## Step 1: Create a function to help recurse over many months of data\n",
        "\n",
        "This function serves as a means to repeatedly run the model to extend the dataset as far as needed. It will accept a dataframe, model, and the number of months to generate."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Functon to extrapolate from dataset (recursive)\n",
        "def extrapolate(df, model, n_month_to_predict, remove_negatives=False):\n",
        "  ''' Takes a dataframe, machine learning model and an integer to represent the\n",
        "  number of months to be predicted.\n",
        "  '''\n",
        "  \n",
        "  # If we are less than 1, just return the last column, we're done\n",
        "  if n_month_to_predict < 1:\n",
        "    return df.iloc[:, -1:]\n",
        "   \n",
        "  # How many columns does the model use?\n",
        "  n_columns = max([s.rank_ for n, s in model.steps if n == 'linearregression'])\n",
        "   \n",
        "  # Then make the prediction for this month\n",
        "  '''\n",
        "    ':'   -> all rows\n",
        "    '-n'  -> count form the back 'n' places\n",
        "    'n:'  -> start at 'n' and go all the way to the end\n",
        "    '-n:' -> start by counting from the end 'n' places,\n",
        "               then take from that position to the end\n",
        "  '''\n",
        "  df[len(df.columns)] = model.predict(df.iloc[:, -n_columns:])\n",
        "\n",
        "  # It makes sense for the data, remove negative predictions\n",
        "  # Set them to zero\n",
        "  if remove_negatives:\n",
        "    df[df < 0] = 0\n",
        "\n",
        "  # Then ask for the next month\n",
        "  return extrapolate(df, model, n_month_to_predict - 1, remove_negatives)\n"
      ]
    },
    {
      "source": [
        "## Step 2: Generate Data\n",
        "\n",
        "- 10 years (120 months) of generated data."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a copy of the dataframe\n",
        "zillow_corrected_pred = zillow_corrected.copy()\n",
        "\n",
        "# Extrapolate will tack on \"new data\" to the dataframe\n",
        "# zillow_corrected_pred will have all of the predictive data in it\n",
        "extrapolate(zillow_corrected_pred, model, 120, True)\n",
        "\n",
        "# Isolate all the predictive data\n",
        "zillow_corrected_pred = zillow_corrected_pred.iloc[:, len(zillow_corrected.columns):]"
      ]
    },
    {
      "source": [
        "Currently the data frame has each month as an integer (84, 85, 86 etc.). These column months need to be changed into an easier to read `YYYY-MM` format. Also each 12 months needs to be averaged out for that year.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Yearly averages function\n",
        "def average_years(df, years):\n",
        "  ''' Averages the data for specific years and returns as new dataframe\n",
        "  '''\n",
        "\n",
        "  yearly_df = pd.DataFrame(index=df.index)\n",
        "  for y in years:\n",
        "    yearly_df[y + '_Average'] = np.round(df[[col for col in df if y in col]].mean(axis=1), 2)\n",
        "    \n",
        "  return yearly_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up columns as dictionary\n",
        "new_columns = dict()\n",
        "\n",
        "# Find the first year\n",
        "first_year = min(list({int(col[:4]) for col in zillow_corrected.columns}))\n",
        "\n",
        "# Make new associations from integers to `YYYY-MM` formatted strings\n",
        "for m in zillow_corrected_pred.columns:\n",
        "   new_columns[m] = str(first_year + int(m / 12)) + '-' + str((m % 12) + 1).zfill(2)\n",
        "\n",
        "# Replace the column names\n",
        "zillow_pred = zillow_corrected_pred.rename(columns=new_columns)\n",
        "\n",
        "# Find all the years in prediction dataset\n",
        "years = list({col[:4] for col in zillow_pred.columns})\n",
        "years.sort()\n",
        "\n",
        "# Save the averages in a new dataframe\n",
        "zillow_year_pred = average_years(zillow_pred, years)"
      ]
    },
    {
      "source": [
        "## Step 3: Export\n",
        "\n",
        "The data is organized by defualt to have every row be a unique `city_state` and every column to have a specific `year-month`. This won't allow the table to expand easily if a user requests a brand-new year-month. Rearanging the data to have a column for place, time, and price will allow for faster lookups. We can also do this for year averages."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export function \n",
        "def export_format(df):\n",
        "  ''' Make the data into an easily suported shape for the server '''\n",
        "\n",
        "  # New dataframe\n",
        "  exportdf = pd.DataFrame(columns = ['City_State', 'Year_Month', 'Price'])\n",
        "   \n",
        "  # Break up the original dataframe by rows, and keep track of the index (i)\n",
        "  # i is the city_state string\n",
        "  for i, row in df.iterrows():\n",
        "    # Go through each column of each row\n",
        "    for col in df.columns:\n",
        "      # Add a new record to the bottom of the new dataframe\n",
        "      exportdf.loc[len(exportdf.index)] = [i, col, row[col]]\n",
        "\n",
        "  return exportdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the historic information we used for the modeling\n",
        "historic = export_format(zillow_corrected)\n",
        "historic['Type'] = 'Historic'\n",
        "\n",
        "# Add the averages that were calculated\n",
        "historic_avg = export_format(zillow_year_average)\n",
        "historic_avg['Type'] = 'Calculated'\n",
        "export = historic.append(historic_avg)\n",
        "\n",
        "# Convert the predicted information\n",
        "pred = export_format(zillow_pred)\n",
        "pred = pred.append(export_format(zillow_year_pred))\n",
        "pred['Type'] = 'Predicted'\n",
        "\n",
        "# Mash it all together\n",
        "export = export.append(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check out export version\n",
        "export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check out yearly version\n",
        "zillow_year_pred"
      ]
    }
  ]
}